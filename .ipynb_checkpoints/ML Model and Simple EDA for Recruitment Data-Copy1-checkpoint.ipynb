{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8583204",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64542402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59abead5",
   "metadata": {},
   "source": [
    "# Import Data Set and Early Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9513c062",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'recruitment_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrecruitment_data.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'recruitment_data.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('recruitment_data.csv') # Load Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f79a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() #See Data 5 row of the data ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52004faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() #Data Set Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differentiate Categorical and Numerical Feature \n",
    "numerical = df.select_dtypes(\"number\").columns.tolist() #Take numerical column name as a list\n",
    "categorical = df.select_dtypes('object').columns.tolist() #Take categorical column name as a list\n",
    "print(numerical)\n",
    "print(categorical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931487d3",
   "metadata": {},
   "source": [
    "# Check for Missing Value and Duplicated Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4931f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for Missing value for each column \n",
    "((df.isnull().sum()/len(df))*100).plot(kind='barh')\n",
    "plt.xlabel('Percentage of Missing Value (%)')\n",
    "plt.ylabel('Column Name')\n",
    "plt.title('Missing Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df809e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for duplicated rows \n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2280aa",
   "metadata": {},
   "source": [
    "**Action:** Because the amount of data that is missing is quite few (Max. Null 8%), then we should drop the missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c9cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop missing data \n",
    "df.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cbc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recheck for Missing value \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d942fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data shape after removing missing values \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06f6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Serial_no unique value \n",
    "df['Serial_no'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b92b024",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since Serial_no column is unique in every row, we could drop the column because it has no meaning \n",
    "df.drop('Serial_no', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6efcc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for Used Column \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd3084",
   "metadata": {},
   "source": [
    "# Simple Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5595d28",
   "metadata": {},
   "source": [
    "## Univariate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfee7e",
   "metadata": {},
   "source": [
    "### Categorical Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f47ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Value Counts of Categorical Column\n",
    "fig = plt.figure(figsize=(20,7))\n",
    "ax_value = []\n",
    "for x in range(len(categorical)): \n",
    "    ax_value.append(f'ax{x}')\n",
    "                    \n",
    "for i in range(len(categorical)):\n",
    "    ax_value[i] = fig.add_subplot(1, len(categorical), i + 1)\n",
    "    df[categorical[i]].value_counts().plot(kind='bar', color='red', ax=ax_value[i])\n",
    "    ax_value[i].set_title(f'count of {categorical[i]}')\n",
    "    ax_value[i].set_xlabel(f'{categorical[i]} Column')\n",
    "    ax_value[i].set_ylabel('Value Counts')\n",
    "    plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c1234f",
   "metadata": {},
   "source": [
    "**Conclusion :** Most of the candidate didn't have any prior experience for python as well as internship. The job was dominated by male and most of the candidate has a Degree (Graduate). \n",
    "\n",
    "Based on the target (Recruitment_Status), we can identify that the data set is imbalanced data set. Thus, Imbalanced Data Handling is necessary to be done before training the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21b7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Summary of Categorical Columns\n",
    "df[categorical].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceda6648",
   "metadata": {},
   "source": [
    "### Numerical Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2f486c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since we have already drop Serial_no column,  we have to create a new list consisting the remaining numerical column \n",
    "numerical_new = df.select_dtypes('number').columns.tolist()\n",
    "numerical_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution Plot for Numerical Column \n",
    "fig = plt.figure(figsize=(20,7))\n",
    "ax_value = []\n",
    "for x in range(len(numerical_new)): \n",
    "    ax_value.append(f'ax{x}')\n",
    "                    \n",
    "for i in range(len(numerical_new)):\n",
    "    ax_value[i] = fig.add_subplot(1, len(numerical_new), i + 1)\n",
    "    df[numerical_new[i]].plot(kind='kde', color='Blue', ax=ax_value[i])\n",
    "    df[numerical_new[i]].plot(kind='hist', density=True, bins=16, color='orange', ax=ax_value[i])\n",
    "    ax_value[i].set_title(f'{numerical_new[i]} Distribution')\n",
    "    ax_value[i].set_xlabel(f'{numerical_new[i]} Column')\n",
    "    ax_value[i].set_ylabel('Value Counts')\n",
    "    plt.tight_layout()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4575389",
   "metadata": {},
   "source": [
    "**Conclusion:** Experience Years and Offer History column are discrete numerical variable. While Score and Salary (continous variable) seems to be skewed distributed data. Thus, we will expect there are outliers in Score and Salary Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ffdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Summary of Numerical Feature \n",
    "df[numerical_new].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boxplot for Continuous Numerical Column\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "continous_numerical = ['Score', 'Salary * 10E4']\n",
    "ax_value = []\n",
    "for x in range(len(continous_numerical)): \n",
    "    ax_value.append(f'ax{x}')\n",
    "                    \n",
    "for i in range(len(continous_numerical)):\n",
    "    ax_value[i] = fig.add_subplot(1, len(continous_numerical), i + 1)\n",
    "    df[continous_numerical[i]].plot(kind='box', color='orange', ax=ax_value[i])\n",
    "    ax_value[i].set_title(f'{continous_numerical[i]} Box Plot')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366c832",
   "metadata": {},
   "source": [
    "**Possible Action** : Since Score is achieved from candidate test and interview score, it is make sense that there are some candidates that are brilliant. For the salary, it is also make sense for candidates who have a bigger previous salary because of the his/her experience. So, this two outliers will be kept for the modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62add3",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf4a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the Correlation Matrix for Numerical Column\n",
    "plt.figure(figsize=(15,10))\n",
    "correlation = df.corr()\n",
    "sns.heatmap(correlation, annot=True, fmt='.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38df02a4",
   "metadata": {},
   "source": [
    "**Conclusion** : There is no column which has a correlation value greater than 0.8. Thus, we can carry on with all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cad7433",
   "metadata": {},
   "source": [
    "# Pre Processing Categorical Variable (Categorical ---> Numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c09fd91",
   "metadata": {},
   "source": [
    "## Python_exp and Internship Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65297b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478fbd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Internship'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b64cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Python_exp'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0344bd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Endocing Python_exp and internship column \n",
    "y_n_enc = {'Yes' : 1, \n",
    "          'No' : 0}\n",
    "df['Internship'] = df['Internship'].map(y_n_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9ecea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Python_exp'] = df['Python_exp'].map(y_n_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313de523",
   "metadata": {},
   "source": [
    "## Recruitment_Status Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recruitment_Status Unique Value \n",
    "df['Recruitment_Status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Encoding for Recruitment_Status Column \n",
    "rec_enc = {'Y' : 1, \n",
    "          'N' : 0}\n",
    "df['Recruitment_Status'] = df['Recruitment_Status'].map(rec_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1800f8b",
   "metadata": {},
   "source": [
    "## Gender Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c3e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Column Unique Value \n",
    "df['Gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e23acdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Encoding for Gender Column \n",
    "gen_enc = {'Male' : 1, \n",
    "          'Female' : 0}\n",
    "df['Gender'] = df['Gender'].map(gen_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371086b3",
   "metadata": {},
   "source": [
    "## Education Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c77e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Education Column Unique Valie \n",
    "df['Education'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary Encoding for Education Column \n",
    "ed_enc = {'Graduate' : 1, \n",
    "          'Not Graduate' : 0}\n",
    "df['Education'] = df['Education'].map(ed_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92816fe3",
   "metadata": {},
   "source": [
    "## Location Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3458d4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Location Unique Value \n",
    "df['Location'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d19236",
   "metadata": {},
   "source": [
    "**Possible Action:** Because candidates who live in Urban Place has a higher tendency to become a top candidate (Urban Education is better than rural and also its has better competitiveness). Thus, we should perform ordinal encoding for this column.   \n",
    "\n",
    "Rank for Ordinal Encoding: \n",
    "\n",
    "Urban (Highest) -> Semiurban -> Rural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc7a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordinal Encoding for Location Column \n",
    "loc_enc = {'Urban':3, \n",
    "          'Semiurban' : 2, \n",
    "          'Rural' : 1}\n",
    "df['Location'] = df['Location'].map(loc_enc)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb905d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Preprocessed Data Set\n",
    "from IPython.display import FileLink\n",
    "# Import a module to delete the file\n",
    "import os\n",
    "# Create a download function\n",
    "def csv_download_link(df, csv_file_name, delete_prompt=True):\n",
    "    \"\"\"Display a download link to load a data frame as csv within a Jupyter notebook\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas data frame\n",
    "    csv_file_name : str\n",
    "    delete_prompt : bool\n",
    "    \"\"\"\n",
    "    df.to_csv(csv_file_name, index=False)\n",
    "    display(FileLink(csv_file_name))\n",
    "    if delete_prompt:\n",
    "        a = input('Press enter to delete the file after you have downloaded it.')\n",
    "        os.remove(csv_file_name)\n",
    "\n",
    "\n",
    "# Use the function to diplay a download link\n",
    "csv_download_link(df, 'cleaned Recruitment.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2047ccf5",
   "metadata": {},
   "source": [
    "# Modelling and Best Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89c03b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Target and Feature \n",
    "X = df.drop('Recruitment_Status', axis=1) #Feature\n",
    "Y = df['Recruitment_Status'] #Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4cb99ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perform SMOTE Imbalanced Data Handling \n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "X_res, y_res = smote.fit_resample(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c183f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_res.shape, y_res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554f9025",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split Train Test Data \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.2, random_state=42) #Split by 80% and 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af59acc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d2123a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Modelling and Evaluate using AUC Score \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "model_alg = [RandomForestClassifier(), \n",
    "             DecisionTreeClassifier(),\n",
    "             KNeighborsClassifier(), \n",
    "             LogisticRegression() \n",
    "             ]\n",
    "df_mod_eval = pd.DataFrame(columns = ['Algorithm','AUC Score']) \n",
    "\n",
    "for model in model_alg : \n",
    "  model.fit(X_train, y_train)\n",
    "  algorithm = str(type(model)).split('.')[-1][:-2]\n",
    "  y_pred_proba = model.predict_proba(X_test)[::,1]\n",
    "  auc = roc_auc_score(y_test, y_pred_proba)\n",
    "  df_mod_eval = df_mod_eval.append({'Algorithm' : algorithm, \n",
    "                                    'AUC Score' : auc},ignore_index=True)\n",
    "df_mod_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c87d895",
   "metadata": {},
   "source": [
    "**Conclusion :** Random Forrest Classifier Gives the Best Model for the Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e5bec",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaeee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd72b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report \n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = rfc.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c76b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ROC Curve \n",
    "y_pred_proba = rfc.predict_proba(X_test)[:][:,1]\n",
    "\n",
    "df_actual_predicted = pd.concat([pd.DataFrame(np.array(y_test), columns=['y_actual']), pd.DataFrame(y_pred_proba, columns=['y_pred_proba'])], axis=1)\n",
    "df_actual_predicted.index = y_test.index\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "fpr, tpr, tr = roc_curve(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])\n",
    "auc = roc_auc_score(df_actual_predicted['y_actual'], df_actual_predicted['y_pred_proba'])\n",
    "\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f' %auc)\n",
    "plt.plot(fpr, fpr, linestyle = '--', color='k')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5012b5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix \n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,7))\n",
    "target_pred = rfc.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, target_pred, labels=rfc.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                             display_labels=rfc.classes_)\n",
    "disp.plot(ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a369dc6d",
   "metadata": {},
   "source": [
    "**Conclusion :** The Model has Sufficient Accuracy (88%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796cd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance \n",
    "arr_feature_importances = rfc.feature_importances_\n",
    "arr_feature_names = X_train.columns.values\n",
    "    \n",
    "df_feature_importance = pd.DataFrame(index=range(len(arr_feature_importances)), columns=['feature', 'importance'])\n",
    "df_feature_importance['feature'] = arr_feature_names\n",
    "df_feature_importance['importance'] = arr_feature_importances\n",
    "df_all_features = df_feature_importance.sort_values(by='importance', ascending=False)\n",
    "df_all_features.set_index('feature', inplace=True)\n",
    "df_all_features.plot(kind='barh')\n",
    "plt.title('Feature Importance')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b581e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
